{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "\n",
    "<img src=\"http://www.cs.cornell.edu/courses/cs4780/2015fa/web/projects/03NaiveBayes/nb.png\" alt=\"naive_bayes\" style=\"width:350px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes methods are a set of supervised learning algorithms based on **applying Bayes’ theorem with the “naive” assumption of conditional independence between every pair of features** given the value of the class variable. Bayes’ theorem states the following relationship, given class variable $y$ and dependent feature vector $x_1$ through $x_n$, :\n",
    "\n",
    "- $x_1, ..., x_n$ : feature\n",
    "- $y$ : output class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(y \\mid x_1, \\dots, x_n) = \\frac{P(y) P(x_1, \\dots x_n \\mid y)}\n",
    "                                 {P(x_1, \\dots, x_n)}$$\n",
    "                                 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the naive **conditional independence assumption** that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(x_i | y, x_1, \\dots, x_{i-1}, x_{i+1}, \\dots, x_n) = P(x_i | y),$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for all $i$, this relationship is simplified to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(y \\mid x_1, \\dots, x_n) = \\frac{P(y) \\prod_{i=1}^{n} P(x_i \\mid y)}\n",
    "                                 {P(x_1, \\dots, x_n)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since $P(x_1,...,x_n)$ is constant given the input, we can use the following classification rule:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\\begin{aligned}P(y \\mid x_1, \\dots, x_n) \\propto P(y) \\prod_{i=1}^{n} P(x_i \\mid y)\\\\\\Downarrow\\\\\\hat{y} = \\arg\\max_y P(y) \\prod_{i=1}^{n} P(x_i \\mid y),\\end{aligned}\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we can use Maximum A Posteriori (MAP) estimation to estimate $P(y)$ and $P(x_i | y)$; the former is then the relative frequency of class  in the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Prior ($P(y)$)\n",
    "\n",
    "Just count how many times each class came out, then normalize by sum of all counts\n",
    "\n",
    "```python\n",
    "class_prior = np.log(float(len(class_indices)) / len(train_Y))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design Choices for Naive Bayes ($P(x_i|y)$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only thing we need to decide when designing naive bayes classifier is how to represent $P(x_i|y)$.\n",
    "\n",
    "There are several options depends on the type of each feature.\n",
    "\n",
    "### Discrete-valued feature case ([Bernoulli distribution](https://en.wikipedia.org/wiki/Bernoulli_distribution))\n",
    "\n",
    "Parameters to learn\n",
    "- $P(i \\mid y)$ : 해당 class가 0 과 1이 각각 몇번 나왔는지 세어준 후 전체 횟수로 나눠주면 됨\n",
    "\n",
    "\n",
    "$$\n",
    "P(x_i \\mid y) = P(i \\mid y) x_i + (1 - P(i \\mid y)) (1 - x_i)\n",
    "$$\n",
    "\n",
    "<img src=\"http://bluehawk.monmouth.edu/rclayton/web-pages/s05-525/sapdg2-note.png\" alt=\"bernoulli_distribution\" style=\"width:250px;\"/>\n",
    "\n",
    "```python\n",
    "from scipy.stats import bernoulli\n",
    "\n",
    "class_means = np.mean(class_features, axis=0)\n",
    "prob_function = bernoulli(class_means[idx]).pmf\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous-valued feature case ([Gaussian distribution](https://en.wikipedia.org/wiki/Normal_distribution))\n",
    "\n",
    "Parameters to learn\n",
    "- $\\mu$ : 해당 class의 평균을 구해주면 됨\n",
    "- $\\sigma$ : 해당 class의 분산을 구해주면 됨\n",
    "\n",
    "$$\n",
    "P(x_i \\mid y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_y}} \\exp\\left(-\\frac{(x_i - \\mu_y)^2}{2\\sigma^2_y}\\right)\n",
    "$$\n",
    "\n",
    "<img src=\"http://images.books24x7.com/bookimages/id_5642/fig11-10.jpg\" alt=\"gaussian_distribution\" style=\"width:250px;\"/>\n",
    "\n",
    "```python\n",
    "from scipy.stats import norm\n",
    "\n",
    "class_means = np.mean(class_features, axis=0)\n",
    "class_stds = np.std(class_features, axis=0)\n",
    "prob_function = norm(class_means[idx], class_stds[idx]).pdf\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's check the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import csv\n",
    "from operator import itemgetter\n",
    "\n",
    "from scipy.stats import norm, bernoulli\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(train_fname, test_fname, features_to_use=None):\n",
    "    def _read_dataset(fname):\n",
    "        X = []\n",
    "        Y = []\n",
    "        with open(fname, 'r') as fp:\n",
    "            reader = csv.reader(fp)\n",
    "            for idx, line in enumerate(reader):\n",
    "                if idx == 0:\n",
    "                    continue\n",
    "\n",
    "                if features_to_use is None:\n",
    "                    X.append(line[1:])\n",
    "                else:\n",
    "                    X.append(itemgetter(*features_to_use)(line[1:]))\n",
    "                Y.append(line[0])\n",
    "        X = np.array(X, np.float32)\n",
    "        Y = np.array(Y, np.float32)\n",
    "        return (X, Y)\n",
    "\n",
    "    train_data = _read_dataset(train_fname)\n",
    "    test_data = _read_dataset(test_fname)\n",
    "\n",
    "    return train_data, test_data\n",
    "\n",
    "\n",
    "def compute_accuracy(predictions, answers):\n",
    "    assert len(predictions) == len(answers), \\\n",
    "        \"Num predictions %d and num answers %d does not match\" % (len(predictions), len(answers))\n",
    "\n",
    "    num_correct = 0\n",
    "    for prediction, answer in zip(predictions, answers):\n",
    "        if prediction == answer:\n",
    "            num_correct += 1\n",
    "    return float(num_correct) / len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayesClassifier(object):\n",
    "    def __init__(self, num_classes=2):\n",
    "        self._num_classes = num_classes\n",
    "        self._feature_prob_functions = {}\n",
    "        self._class_prior = []\n",
    "\n",
    "    def fit(self, train_X, train_Y, is_continuous):\n",
    "        for class_id in range(self._num_classes):\n",
    "            class_indices = np.where(train_Y == float(class_id))[0]\n",
    "            class_features = np.take(train_X, class_indices, axis=0)\n",
    "            class_means = np.mean(class_features, axis=0)\n",
    "            class_stds = np.std(class_features, axis=0)\n",
    "            class_prior = np.log(float(len(class_indices)) / len(train_Y))\n",
    "\n",
    "            feature_prob_functions = []\n",
    "            for idx, continuous in enumerate(is_continuous):\n",
    "                if continuous:\n",
    "                    prob_function = norm(class_means[idx], class_stds[idx]).pdf\n",
    "                else:\n",
    "                    prob_function = bernoulli(class_means[idx]).pmf\n",
    "                feature_prob_functions.append(prob_function)\n",
    "            self._feature_prob_functions[class_id] = feature_prob_functions\n",
    "            self._class_prior.append(class_prior)\n",
    "\n",
    "    def predict(self, test_X):\n",
    "        predictions = []\n",
    "        for test_x in test_X:\n",
    "            output_prob = []\n",
    "            for class_id in range(self._num_classes):\n",
    "                cum_prob = self._class_prior[class_id]\n",
    "                for x, prob_function in zip(test_x, self._feature_prob_functions[class_id]):\n",
    "                    cum_prob += np.log(prob_function(x))\n",
    "                output_prob.append(cum_prob)\n",
    "            prediction = np.argmax(output_prob)\n",
    "            predictions.append(prediction)\n",
    "        return predictions\n",
    "    \n",
    "def run_naive_bayes(train_data, test_data, is_continuous):\n",
    "    train_X, train_Y = train_data\n",
    "    test_X, test_Y = test_data\n",
    "\n",
    "    nb_classifier = NaiveBayesClassifier()\n",
    "    nb_classifier.fit(train_X, train_Y, is_continuous)\n",
    "    pred_Y = nb_classifier.predict(test_X)\n",
    "    accuracy = compute_accuracy(pred_Y, test_Y)\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = read_dataset(\"./train_data.csv\", \"./test_data.csv\")\n",
    "is_continuous = [True] * 2 + [False] * 8 + [True] * 2\n",
    "accuracy = run_naive_bayes(train_data, test_data, is_continuous)\n",
    "print(\"Accuracy : {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "- [Naive Bayes in scikit-learn](http://scikit-learn.org/stable/modules/naive_bayes.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
